import firedrake
# We need to import firedrake_adjoint to fire up the taping!
import firedrake_adjoint  # noqa
import torch
import torch.nn as nn
from fecr import evaluate_primal
from firedrake import (DirichletBC, FunctionSpace, SpatialCoordinate, Constant,
                       TestFunction, UnitSquareMesh, assemble, dx, grad, inner)
from torch.autograd import Variable
from torchfire import fd_to_torch

mesh = UnitSquareMesh(3, 2)
V = FunctionSpace(mesh, "P", 1)

# zero on 3 sides
bc = DirichletBC(V, 0, (1, 2, 4))


# This assumes that e^kappa will be computed outside of firedrake
# and stuffed into a piecewise linear FD function.
def assemble_firedrake(u, expkappa):
    x = SpatialCoordinate(mesh)
    v = TestFunction(u.function_space())
    f = Constant(20.0)

    return assemble(inner(expkappa * grad(u), grad(v)) * dx - inner(f, v) * dx, bcs=bc)

# assemble_firedrake just takes a pair of functions now
templates = (firedrake.Function(V), firedrake.Function(V))

class residualLoss(torch.nn.Module):
    def __init__(self) -> None:
        super(residualLoss, self).__init__()

    def forward(self, z, A, u_nn) -> torch.Tensor:
        # Compute kappa at mesh points via A @ z
        kappa = A @ z

        # Exponentiate pointwise to get some ekappa tensory
        expkappa = torch.exp(kappa)

        # Pass ekappa and u through the torchfire-wrapped function to get a vector
        res = fd_to_torch(assemble_firedrake, templates, "residualTorch")
        nn_output = u_nn(z)
        res_ = res.apply(nn_output, expkappa)

        # Return Euclidean norm of that vector
        mse_loss = torch.nn.MSELoss()
        loss = mse_loss(res_, torch.zeros_like(res_))

        return loss


# TODO:
# Write a function that creates a torch.Tensor
# tabulating the KL expansion, calls assemble_firedrake
# and takes the Euclidean norm of the vector.

# Need a function (Van & Jon?) that takes p (order of KL expansion) and a mesh
# (get coordinates from mesh.coordinates.dat.data)
# and returns a matrix A tabulating the KL eigenfunctions scaled by sqrt(lam_i)
# at each grid point.
#
# Then realizing kappa from the z vector is a matrix-vector product.
#
# Our loss function (Jorge!) takes random z and NN prediction u (both torch.tensors) and
# 1.) compute kappa at mesh points via A @ z and
# 2.) exponentiate pointwise to get some ekappa tensory
# 3.) pass ekappa and u through the torchfire-wrapped function to get a vector
# 4.) return Euclidean norm of that vector.

# Afterwards, we can check that this really works and try to train a neural net with it!

# TODO: Replace this Neural Network with the one defined by Van & Jon
class neural_net(torch.nn.Module):

    def __init__(self):
        super(neural_net, self).__init__()
        self.linear1 = nn.Linear(12, 128)
        self.activation1 = nn.LeakyReLU()
        self.linear2 = torch.nn.Linear(128, 12)

    def forward(self, x):
        x = torch.flatten(x)
        x = self.linear1(x)
        x = self.activation1(x)
        x = self.linear2(x)
        return x

# TODO: Replace this z and A by the ones generated by Van & Jon
z = torch.ones(V.dim())
z.requires_grad = True
A = torch.eye(V.dim()) @ torch.rand( (V.dim(),V.dim()) )

# TODO: Replace the Training pipeline with the one defined by Van & Jon
u_nn = neural_net()
learning_rate = 0.001
optimizer = torch.optim.Adam(u_nn.parameters(), lr=learning_rate)
losses = []

# Defining the residual loss by calling our custom loss class `residualLoss`
residual_loss = residualLoss()

for epoch in range(5):
    print(f"Epoch:{epoch}")

    # This computes the residual loss given the tensors z, A and the neural network that generates the solution u_nn
    loss = residual_loss(z = z, A = A, u_nn = u_nn)

    # This step can be deleted. It is just included here to test that the gradients are being computed with no issues
    grad_x, = torch.autograd.grad(loss, z, create_graph=True)
    print(grad_x)

    losses.append(loss)
    print(f"Loss:{loss}")

    # Backpropagation
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
